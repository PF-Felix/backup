# 📦项目信息

> 是在博锐做的商城APP

## 🥈业务组成

`数字马力面试中的业务架构`

> 在业务架构方面，主要关注系统的功能需求、业务流程以及业务特点。通过与业务部门密切合作，梳理了用户需求和业务流程，确定了系统的关键功能模块，比如下面这些。
> 同时，我们还对业务数据进行建模和分析，确保了业务架构的合理性和完整性。
>
> 深刻认识和经验：
> 多年的经验告诉了我深入理解业务需求和业务流程的重要性。在项目初期，我们需要花费更多的时间和精力去梳理和分析业务需求，确保系统的功能设计和业务流程符合用户的需求和预期。同时，我们也需要不断与业务人员进行沟通和协作，及时获取用户的反馈和意见，以便对系统进行优化和改进。

- 用户中心：注册、登录、鉴权、账号安全、用户下线
- 运营用户管理：管理员增删改查运营用户、权限管理
- 商品管理：商品分类、品牌/制造商、属性、商品上架下架
- APP页面：首页、商品分类、搜索、商品列表、商品详情、评价
- 交易：购物车、下单、支付、退货、退款
- 促销：秒杀活动、促销活动、拼团活动
- 红包雨
- 库存管理：采购入库
- 物流系统
- 客服：在线人工、售后系统

## 🥈技术架构

`数字马力面试中的技术架构`

> 在技术架构方面，我们综合考虑了项目的需求、团队的技术实力以及未来的可维护性。
>
> 首先，我们采用了敏捷开发方法，以短周期迭代开发的方式进行项目开发。这使得我们可以快速响应需求变更，提高开发效率和质量。
>
> 为了实现前后端分离，我们采用了 RESTful API 进行前后端交互。前后端分离可以独立开发和测试，降低了耦合度。
>
> 为了降低网络延时，利用了 CDN 来缓存静态资源，减少响应时间
>
> 在数据库方面
> 我们选择 MySQL 做数据持久化存储，通过分库分表和读写分离进行请求分流，实现了高性能的读写。
> 使用 Redis 作为缓存层来提高系统性能。
> 同时还使用了 Elasticsearch 来提供搜索功能，使用 RocketMQ 来实现异步消息处理。
>
> 而且考虑到高可用，将并发量大的容易出现问题的业务与主业务进行服务隔离，避免影响到主业务。
>
> 为了及时处理线上问题，我们有专门的服务实时监控各个微服务的健康状态，出现问题及时报警。
>
> 这些技术的综合应用使得我们的系统具有良好的性能和可扩展性。
>
> 在数据安全方面
> 存储安全：对于必须要解密的数据，比如密码，采用散列算法加密，不用担心被逆向破解；对于需要解密的数据，采用可逆的对称加密算法，比非对称加密更快
> 传输安全：使用了 https 保证的数据的防泄密和防伪造，而且针对需要响应给用户的敏感数据进行脱敏处理后再响应

首先项目是**动静分离**的

静态资源都存储在**CDN**加快了静态资源的访问速度

源站采用**微服务架构**
项目按照功能以及其他边界拆分成了多个应用，对流量进行了分流
每个应用都是一个无状态集群，方便横向扩展，进行了进一步分流，集群内服务越多能够支撑的流量就越大

**Nginx**做流量网关，做负载均衡与流量转发，搭配 keepalived 实现高可用

**gateway**是业务网关，做鉴权等工作
为什么用了 gateway 还要用 nginx？
因为 gateway 的性能远不及 nginx，使用 nginx 能接入更多的流量，再分发给 gateway 集群

**Nacos**作注册中心和配置中心

**Sentinel**用作限流、熔断和降级，使用压测的数据做限流

链路监控用**skywalking**

**Redis集群**作分布式缓存

**RocketMQ**用作解耦、异步、削峰、最终一致性等场景

数据持久化用**MySQL**主从架构

商品搜索引擎使用**ES**

**分布式定时任务调度**用XXL-JOB
**分布式ID**利用两台Redis实现
**分布式事务**用Seata
**分布式锁**用Redisson

对象云存储使用的**阿里云OSS**

运维部署使用**K8S**

## 🥈服务划分

`数字马力面试中的系统架构`

> 在系统架构方面，注重模块化、可扩展性和高可用性。
>
> 首先，将系统划分为前端用户界面、后端业务逻辑和数据存储三个主要模块。前端用户界面负责与用户交互，后端业务逻辑处理业务逻辑，数据存储负责数据的读写。
>
> 为了提高系统的可扩展性，我们采用了微服务架构，将各个功能模块划分为独立的微服务。这种架构使得每个微服务都可以独立开发、测试和部署，大大提高了开发效率。
>
> 同时，我们还采用了云原生容器化技术（Docker、K8S）实现了快速部署和水平扩展。
>
> 为了确保高可用性，我们采用了负载均衡和容错机制。通过配置负载均衡器，我们将请求分散到多个服务器上，确保了系统的吞吐量和响应速度。同时，容错机制使得当部分服务器发生故障时，系统仍能保持正常运行。

- 网关服务
- 用户服务
- 商品服务（包括购物车、评价）
- 搜索服务
- 秒杀服务
- 促销服务
- 库存服务
- 订单服务
- 第三方接入服务
- 客服服务

## 🥈用户量+数据量+并发量+硬件配置

后期的用户量有15W，日活1W+，QPS 在 2000 左右，这个配置足以满足日常的需求

以商品中心举例：服务器配置是 CentOS7 2C4G，压测结果是一台服务可以支撑 1500QPS，考虑到 2000QPS 我们使用了三台服务器

**QPS**

~~1000QPS 是什么水平？~~

一些简单的网站或者小型应用可能只需要处理几十个QPS
一些中等规模的应用可能需要处理几百到几千的QPS
而一些大型应用则需要处理数万个甚至更多的QPS

因此，1000QPS 能够满足一些中等规模的应用需求，但对于一些大型应用来说，可能需要更高的QPS

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231007093736903.png" alt="image-20231007093736903" style="zoom:70%;" />

~~2C4G能支持多少QPS~~

一般而言，这样的服务器能够支持大约几百到几千的QPS

~~QPS如何估算~~

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231007093859435.png" alt="image-20231007093859435" style="zoom:80%;" />

## 🥈项目人员+开发周期

**这个如果不问就不说**

项目人员：研发负责人、后端开发、安卓/IOS、UI、测试、运维；总人数20+

项目上线用了2个月

# 其他项目经验

> 文档编写和知识积累的重要性。包括需求文档、设计文档、测试文档、代码注释等。通过这些文档和资料能更好地理解项目的过程和结果，为未来的项目提供宝贵的经验和支持。

> 不断学习和自我提升的重要性。不断学习和尝试新的技术和方法，提高技术水平和解决问题的能力。

# 🧇数据表设计

查看语雀：[尚品APP数据表设计](https://www.yuque.com/felix.y/fhtohl/tg1gz5z96r912eci)

**商品中心**

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231014083636831.png" style="zoom:36%;" />

**库存管理**

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231018141744522.png" alt="image-20231018141744522" style="zoom: 40%;" />

**促销中心**

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231018141834039.png" alt="image-20231018141834039" style="zoom:40%;" />

# 🧔用户中心

## 🥈发送短信验证码

1、调用【第三方接入服务】发送短信成功，短信内容为随机验证码
2、然后把【验证码+当前时间】作为 value，【固定前缀+手机号】作为 key 存入 redis，过期时间是5分钟

**为了防止恶意流量，设计的是1分钟内不允许重复发送短信**

发短信前，根据手机号从 redis 取出 value，如果有且解析得到的【当前时间】与当前时间差小于一分钟，就提示用户短信发送过于频繁稍后再试

## 🥈注册流程

1. 校验提交信息的合法性，比如空值判断
2. 校验验证码（如果验证成功就从 redis 删除）
3. 校验用户名和手机号是否重复
4. 加密密码后，插入数据库
5. 返回成功

## 🥈用户登录

分布式会话可以考虑的方案：分布式session 和 Token

### 🥉分布式session

- 使用 redis 实现 session 共享
- 用 session 必须 cookie 配合，浏览器支持 cookie，但原生APP不支持，而用 Token 更加简单

### 🥉Token有两种方案

#### ✨服务端不存Token

不存就必须给 Token 设置过期时间

【典型例子】就是 JWT，即 json web token，JWT 分为三个部分：

1. 第一部分是算法本身转化的 Base64 字符串
2. 第二部分是用户信息转化的 Base64 字符串
3. 第三部分是使用算法和盐对前两个部分的加密（无法逆向破解的算法）

【安全性】别人不知道我的算法和盐就无法伪造正确的 Token

【优点】服务端免去了存储 Token 的负担，而且解析 Token 比查共享存储验证权限速度快

【缺点】不能及时关闭对用户的授权，只能依赖于 Token 过期；用户 Token 被盗就麻烦了
【解决方案】就是添加 Token 黑名单，但这依赖于共享存储，违背了不存的初衷，因此不合适

因此不存 Token 适用于对安全性要求不高的场景

#### ✨服务端存Token

想关闭授权删除用户的 Token 即可，用户需要重新登录才能取得授权

**共享存储的选择**

不推荐用数据库性能较低，可以使用高可用的 Redis 集群

**Redis宕机？**

宕机是偶发现象，而且即使宕机造成部分 Token 丢失影响的用户量也不会很大，完全可以接受

**双 Token 提升用户体验**

一旦 Token 到期从 Redis 中删除，用户就得重新登录，这样会影响用户体验

每次请求都更新 Token？如果请求断了没收到响应，用户就得重新登录

我优化的方案是使用双 Token，用户登录成功之后下发双 Token：accessToken 和 refreshToken

- accessToken 用于接口调用
- refreshToken 用于 Token 刷新：如果调用接口发现 accessToken 过期，就调用刷新 Token 接口得到新的双 Token，再利用新的 accessToken 再次调用原接口，因此 refreshToken 过期时间比 accessToken 长一些

**关闭授权**

同时删除双 Token 即可

**另外为了用户安全，防止暴力破解**

如果密码 or 验证码多次校验失败，当天禁止用户登录，利用 Redis 计数器实现

**密码安全**

散列算法不可逆不会被逆向破解，对称加密和非对称加密可以被逆向破解
因为密码没有解密的需求，因此使用散列算法，更加安全

# 🛍️商品中心

## 🥈商品分类

### 🥉后台分类

商城APP 使用的是树状的分类

用户是经常访问分类的，如果每次都查数据库显然是不行的，影响用户体验，而且请求量大的时候对数据库造成压力

我想到了两种方案

**方案1**APP写死，不再请求服务端获取分类信息

优点是释放了服务端的压力
缺点是无法动态更新分类

**方案2**用 Redis 缓存

~~缓存预热~~

服务启动时缓存预热，写一个 Bean 实现 CommandLineRunner 或 ApplicationRunner 接口，在 run() 中从数据库中查询分类信息并存入 Redis

~~增删改分类信息的缓存一致性问题~~

双写一致性可以参考《三高架构》

双写一致性的最优方案也可能出现不一致的情况，虽然概率很低，有下面几种解决方案：

1. 加锁保证写的顺序，能保证强一致，但不适合并发量大的场景
2. 如果数据一致性要求不高，加上一个过期时间即可
3. 如果数据一致性要求高，用 canal 订阅数据库 binlog 更新缓存，落库顺序与更新缓存顺序一致

**方案选型**

- 【方案1】
  - 是实现最简单且性能最高的方案，满足业务需求的话肯定采用
  - 但是~~项目初期~~，不断有新分类加入的需求，用户不更新 APP 就看不到新分类，影响商品销售
  - 因此没有使用方案1，而是使用的方案2，而【方案1】考虑在后期分类稳定的时候或许可以采用
- 【方案2】
  - 【1】不合适
  - 【3】设计有点复杂了而且实现起来挺麻烦
  - 我们采用【2】满足需求且实现简单；需要注意的是【2】要注意预防缓存带来的风险：缓存穿透、击穿、雪崩

上面的【2】在缓存失效后，大量并发还是会阻塞等待

### 🥉前台分类

- 后台分类的层级到了三级，已经不适用于移动端了，操作太深影响用户体验
- 后台分类是基础数据不允许随便改动
- APP商品分类不一定要用后台分类的名称，应该选择适合导购的名称
- APP商品分类需要动态变化，比如服装类，季节不同商品分类的排序是不同的，夏季肯定把夏装放在前面
- APP商品分类还有聚合后台分类的需求，将有通用属性的商品聚合展示，比如将后台分类的连衣裙、长裙聚合为前台分类女裙
- 特殊分类：给每个类型分配一个标志位，搜索服务根据请求的标志位做搜索
  - 新品：没有关联的后台分类，搜索上架时间小于两周的商品
  - 秒杀促销：请求秒杀服务从缓存中获取商品列表
  - 满减促销：请求的是搜索引擎

## 🥈品牌/制造商

商品的来源有的是采购的品牌商品，有的是直接找到的某品牌的制造商

品牌是在类目树的叶子节点下创建的，品牌与商品分类是多对多的关系

## 🥈SPU&SKU&属性

**SPU**

全称是 Standard Product Unit，标准产品单元
通俗点讲，基本属性相同的商品就可以称为一个SPU

京东举例如下图：

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1685173735566-d482242a-9df5-4d61-90c2-3c2f9dcd5fb1.png" alt="image.png" style="zoom:67%;" />

**SKU**

全称是 stock keeping unit，库存量单位

还是用京东举例：

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1685185379448-567e5d37-bfe1-4236-9f7d-7d4ac01f1c3c.png" alt="image.png" style="zoom:67%;" />

**基本属性&销售属性**

基本属性就是 SPU 对应的公共属性
销售属性是 SKU 特有的属性

**属性组**

PS：这个项目没有用到属性组
属性组需要和基本属性关联

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1683840607170-f668d3bb-61a1-4a2e-8d8b-e4f81ebeb120.png" alt="img" style="zoom: 96%;" />

## 🥈商品详情

我们做商品详情功能的时候，参考了淘宝和京东的效果

**淘宝**切换SKU 的时候，浏览器地址没有变化，页面没有刷新，商品标题也没有变化
**京东**切换SKU 的时候，浏览器地址有变化，页面有刷新，商品标题也在动态切换为 SKU 的标题

**淘宝的优点**请求服务端的次数少很多，能极大的减轻服务端的压力
**京东的优点**商品的描述更加精细化
`这算是一个考量`

`另外一个考量`是**搜索分页**，根据实际效果以及上面的描述
**淘宝**SKU标题都一样，应该是共用的 SPU 标题，因此 ES 索引是 SPU 维度的，不存在搜索数据重复的问题
**京东**搜索得到的是具体的 SKU 标题
（京东）如果 ES 索引是 SKU 维度的，那搜索得到的结果可能出现重复数据，即 SKU 不同但 SPU 相同，可以是使用 collapse 折叠去重
（京东）如果 ES 索引是 SPU 维度的，每条索引数据里面存放多个 SKU，怎么做到搜索动态匹配 SKU 呢？这个好像做不到

`结合上面的考量`，以及我们并不需要非常的精细化描述，用淘宝方式的效果完全可以，而且性能好

PS：为了商品详细描述部分的美观，设计的是不让输入文字，只能上传多张图片，数据库表用一个字段存储了多张图片的地址以逗号分隔

PS：根据 SPU_ID 查询商品详情使用**异步**加速查询

## 🥈商品发布&上架

### 🥉发布

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1685184818313-197486aa-8ccc-4de0-9357-649d438db7e9.png" alt="image.png" style="zoom: 67%;" />

根据【商品分类ID】查询所有的【基本属性】

![image.png](https://raw.githubusercontent.com/PF-Felix/ImageA/main/1685160890476-7aca726a-53ef-4df7-a350-bc404b6ecf7e.png)

根据【商品分类ID】查询所有的【销售属性】

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1685161310553-ee232b5d-c9fa-4377-8c03-10c73997d071.png" alt="image.png" style="zoom:77%;" />

最后保存：
1. 保存 SPU 基本信息、详细信息、图集
2. 保存商品基本属性（规格参数）
3. 保存 SKU 信息
4. 保存商品销售属性

### 🥉上架

1. 将 SPU 信息存入 ES（调用搜索服务）
2. 更新 SPU 为上架状态

## 🥈搜索服务&商品列表

### 🥉索引结构

方括号是非索引字段，不用于搜索，而是用于搜索分页列表展示

- 商品分类ID
- 品牌ID
- 【品牌名称】
- 【品牌logo】
- 【SPU_ID】
- SPU标题（ik分词器）
- 【SPU默认图片】
- SPU价格（存各个SKU最低的价格）
- 是否有库存
- 上架时间
- 销量
- 商品所有属性ID列表（包括基本属性和销售属性）

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231018105059442.png" alt="image-20231018105059442" style="zoom: 43%;" />

### 🥉分页列表的展示顺序

- 综合：没有任何字段的排序，按评分顺序
- 销量：按销量字段排序
- 上新：按上架时间排序
- 价格：按价格字段排序

### 🥉分页列表的筛选维度

- 按搜索栏输入的词条匹配 SPU 标题
- 价格区间：使用价格字段
- 商品分类
- 品牌，每个分类（叶子）下都有一组品牌，点击分类将带出这些属性选项
- 属性ID，每个分类（叶子）下都有一些公共的可搜索属性，点击分类将带出这些属性选项，比如衣服的尺码（销售属性）、颜色（销售属性）、男女（基本属性）

### 🥉其他点

- 搜索历史存在本地
- 根据输入文字实时搜索热门词条
- ES搜索语法请查看：[尚品APP搜索DSL (yuque.com)](https://www.yuque.com/pf_felix/rrb6od/sflfbc7ar6wnhrz3)

## 🥈购物车

**登录之前允许加入购物车吗？**

No，加入购物车必须登录

**购物车数据放在缓存还是数据库？**

缓存吧，CRUD速度快一些，而且购物车数据没那么重要，意外丢失点数据也是可以接受的

**购物车应该展示哪些信息？**

- 商品信息：SPU+SKU 的基本信息、购买数量、总价、参与的促销活动、展示顺序、是否选中
- 汇总选中的商品计算总价

**购物车什么时候更新？**

1. 用户主动修改
2. 如果商品库存不足或者商品下架，应该及时提醒用户商品的状态，这算是被动更新
3. 如果商品价格修改了，最好的体验是能够看到最新的价格，这也算是被动更新

被动更新的实现方式可以采用 用户进入购物车时获取最新数据

**所有这些都存储缓存吗？**

没必要，缓存中只存储 SKU_ID、SKU_编码、购买数量、排序、是否选中

其他信息进入购物车时一并获取就行了

-----

**进入购物车**

- 根据 SKU_ID 从商品中心获得商品基本信息（SPU+SKU）、上架/下架状态
- 根据 SKU 编码从库存中心获得库存
- 根据 SKU_ID 获得商品参数的所有促销活动
- 前面的几个操作使用【异步】提升性能，最后汇总计算总价

**添加购物车**需要检查库存，库存不足时提示用户

**修改购物车（增加购买数量）**也需要检查库存，库存不足时数量不变

**结算页？**没有结算页

-----

在购物车中支付前必须填写收货地址、订单备注、选择支付方式（类似拼多多的多多买菜）

单个商品点购买，选择SKU+选择收货地址+订单备注+选择支付方式，再支付

提交订单

点立即支付即提交订单

# 🔥秒杀

## 🥈场景&瓶颈&关注点

**秒杀将会产生大量的刷新商品详情页的操作，对商品中心会造成一定的负担**

针对这种场景可以关注的点有：

- 动静分离，静态资源就不要请求后端服务集群了，越往前放越好，比如放在网关
- 客户端优化，参考 [三高架构](https://www.yuque.com/felix.y/dns6b6/fdcge7xpisil11zd)
- 使用CDN，参考《三高架构》
- 使用缓存，把商品详情所需内容缓存起来，避免请求数据库

**为了避免超卖的问题，一般的下订单扣库存都是加分布式锁排队更新数据库的，这就不适合秒杀了，因为秒杀的并发量很大会卡住影响用户体验**

针对这种场景可以关注的点有：

- 使用缓存暂时代替数据库，加快业务处理速度
- 异步处理，快速响应

**如果有恶意流量频繁下单不支付，或者支付了后续都申请退款，很可能影响商品的销售，因为扣除的库存短时间内无法恢复，如果商品卖光了，其他用户想买却买不到**

针对这种场景可以关注的点有：

- 人机交互，让其付出代价，比如下单前需要输入验证码
- 设置只能卖一件，一般的用户也不会买多件
- 将超时未支付自动取消的时间缩短，尽快恢复库存
- 取消订单三次就不允许再购买了

**其他**

- 隔离：将秒杀业务作为独立的服务部署，避免影响到主业务，下面会讲到隔离的使用场景

## 🥈业务流程

### 🥉活动上线

1. 新建秒杀活动
2. 添加参与秒杀活动的商品，参与了秒杀的商品，不能再参与其他的秒杀场次
   添加商品锁库存，库存不够无法添加
3. 定时任务扫描未开始的最近的三个秒杀活动，如果未上线就将活动上线
   1. 更新秒杀活动的状态为上线，上线成功就继续操作
   2. 给 RocketMQ 发消息，发送失败就回滚
4. 消费者监听到消息就做下面的操作，下面操作都是异步
   1. 将未上架的秒杀商品上架，保证能被搜索引擎搜索到
   2. 查询秒杀活动信息存入 Redis
      KV 是 【秒杀活动+秒杀活动ID】/【活动信息】
      这里以及下面的过期时间都是活动结束后5分钟
   3. 查询活动关联的所有 SKU 详细信息封装到 Redis 中
      KV 是【秒杀商品列表+秒杀活动ID+SKUID】/【SKU详细信息+活动开始和结束时间】
   4. 查询 SKU 库存作为信号量加入缓存（Redisson）
      key是【秒杀商品库存+秒杀活动ID+SKU_ID】
   5. 如果所有操作都成功返回成功
      只要有一个操作失败就返回失败，稍后继续消费这个消息
5. 定时任务扫描上线的所有秒杀活动，下线所有过期的活动，下架所有秒杀商品

**为什么不把步骤3+4做成一个过程而是要拆开呢？**为了保证最终一致性

### 🥉客户端展示

查的都是缓存

1. 以【秒杀活动】作为 key 前缀，查到所有的上线的秒杀活动
2. 以【秒杀商品列表+秒杀活动ID】作为 key 前缀，查到商品列表包括详细信息
3. 商品详情中库存查【秒杀商品库存+秒杀活动ID+SKU_ID】

### 🥉秒杀流程

**为了不和其他业务耦合，秒杀商品不支持添加购物车**

1. 点击购买，提交商品信息+收货地址
2. 合法性校验
   1. 活动是否上线 即缓存有没有
   2. 活动是否过期 比较当前时间和商品的过期时间
   3. 购买数量是否超出限购数量
   4. 用Token校验幂等性
3. 获得信号量+扣库存
4. 生成分布式订单号 + 封装订单信息 + 【异步】发送给 RocketMQ + 立即响应
   每秒查一次订单，查到订单之后跳转到支付
5. 【订单服务】监听到消息，快速生成订单（此时用户还未支付）
   异步发送延时消息用来订单超时未支付自动取消，发送失败就重新消费消息
   另一种方案是定时任务扫描，缺点是可能存在较长的延时
   这里只插入订单，不能扣减数据库库存，因为超时未支付恢复库存比较麻烦
6. 支付：支付与修改订单状态应该是一个事务，可以使用分布式事务
   1、更新订单状态为支付中
   2、支付预下单（异常回滚）
   3、支付成功更新订单状态为已支付、失败回滚、无响应就回查
   这里也不能扣减数据库库存，理由同上，会阻塞
   定时任务扫描支付成功的订单扣减数据库库存

## 🥈高可用问题

如果用 Redis 来做限流+快速响应，RocketMQ 异步生成订单
1、单节点的 Redis 会挂
2、使用哨兵主从：Redis 可能出现主从不一致，导致超卖

**那怎么做到高可用呢？**

3、给每个秒杀服务安装一个 Redis，参与秒杀的商品根据ID路由到特定的服务节点处理，即每个服务负担一部分商品的秒杀，即使某个服务或者同机的 Redis 挂掉，也只会影响到一部分商品；缺点是无法做到实时的横向扩展，而且这样做的话秒杀业务的部署就比较复杂了，有必要独立出来，不要影响其他服务

## 🥈RocketMQ相关的问题

### 🥉秒杀step5为什么异步发消息

同步可能长时间阻塞，而秒杀应该做到快速响应，因此异步更合适

**消息发送失败怎么办?**

即便设置了多次重试依然可能失败

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1684476581082-0b49bb0b-cd36-4750-a5f2-f1a1d475e964.png" alt="img" style="zoom: 80%;" />

最终还是发送失败？相当于下单失败了，需要恢复库存

### 🥉同步双写VS异步双写

选同步双写，保证不会因为主节点宕机影响数据一致性

### 🥉消息重复发送的问题

消费者需要做幂等处理

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1684477331236-0007bb36-dcff-4b20-93f6-91fc55698f9c.png" alt="img" style="zoom: 80%;" />

## 🥈超卖和少买的问题

**超卖**

即不能出现最后库存是负数的情况

一般的避免超卖的办法

1. 悲观锁：加互斥锁，有库存才允许扣减
2. 乐观锁+重试

在这里利用 Redisson 信号量（LUA脚本实现）做 库存判断+扣库存 本身就是带锁的，因为执行LUA脚本时不会执行其他脚本和 Redis 命令，因此超卖问题不存在

**少卖**

取消订单要恢复 Redis 库存

# 🧧双11红包雨

**功能上就是两个**

- 发红包：支付 → 创建红包
- 抢红包：红包是否还有余量？
  - 抢光了：不让点了
  - 还有：开红包，更新红包记录

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/1684390210315-30e2f5f1-706c-441d-bae4-c039372bbcfd.png" alt="img" style="zoom: 80%;" />

**数据库表的设计**

两个表，发红包的流程（插红包表）、抢红包的流程（插红包流水表，更新红包表）

**初步思考与设计**

抢红包并发量越大，设计就越复杂，我根据用户量多少给了几个方案

假设一个用户抢红包，需要操作数据库耗时20ms，用户能够接受3s的等待抢到红包

关键是卡在更新红包表这里，并发场景下为避免超卖，检查库存+扣减库存必须得加锁排队

~~如果发放的红包个数是100~~，即允许100个人并发更新，耗时就是2s，能够满足需求

~~如果红包个数是1000~~就难受了

优化方案：

将一个红包拆成10个子红包，开红包时开一个剩余份数最多的红包，或者通过运算将红包id和用户id绑定

降低了锁的粒度，提高了并发量，也就是分流，直流越多，并发量越大

~~1000W的并发量怎么办呢？数据库会挂的~~

使用数据库扣库存慢，用缓存解决

数据一致性问题？
如果要求强一致就又回到了原点因此不可行，可以先将数据持久化到 MQ（比操作数据库快得多）消费者消费消息保证最终一致性

~~由此可见抢红包和秒杀是非常相似的~~

**以微信红包为例**

一个红包就相当于一个商品的秒杀

以微信的用户量，相当于海量的商品同时参与秒杀，因此并发量比电商平台的商品秒杀要大的多

而且微信红包本质上是资金交易，安全级别更高，绝对不允许超卖，少卖（即有红包未被领取）则必须保证精确回款

因此如果架构设计能够满足微信红包的业务场景，那么电商平台的商品秒杀也没问题

【秒杀】中的架构方案已经可以了，将同一个红包的并发路由到一个特定的服务上即可

**以支付宝红包雨为例**

与微信红包不一样的地方是，红包雨就只有一个红包，所有用户参与抢红包

因此这个场景下，对于单个红包的并发量方面，要比微信红包大的多得多

如果用上面微信红包的解决方案

- 假设同时抢红包的人是1000W，单个服务是支撑不住的
- 而且单个服务可用性不高

继续优化：把红包雨的一个红包拆成100份（上文中的思路），特定路由分流给100个服务处理

缺点：客户端不知道哪个红包还有余量，会出现用户没抢到红包但实际上红包还有剩余的情况

**用户收到提示红包抢到了，但实际没有到账，可能是下面的两个原因**

1、Redis 锁库存成功，但是给MQ发消息失败了即数据没有持久化
2、MQ消息过多消费比较慢

# 🌐第三方接入服务

这个服务集成了所有调用第三方的 API，为其他服务赋能

## 🥈发送短信

## 🥈获取阿里云OSS防伪签名

前端在上传图片到阿里云之前，调用这个接口获得上传所需的防伪签名，然后直接调用阿里云OSS的接口上传图片和防伪签名

![image-20231014084619324](https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231014084619324.png)

## 🥈物流能力的接入

## 🥈支付

统一支付宝和微信的 API

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231014084426428.png" alt="image-20231014084426428" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231014084511315.png" alt="image-20231014084511315" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/PF-Felix/ImageA/main/image-20231014084548458.png" alt="image-20231014084548458" style="zoom:67%;" />