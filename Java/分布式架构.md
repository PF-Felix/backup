# 🥇CAP定理&BASE理论

**Consistency（一致性）**

分布式系统中的所有数据备份在同一时刻是否是同样的值

- 如果某个节点更新了数据，其他节点如果都能读取到这个最新的数据就称为强一致
- 如果有某个节点没有读取到就是不一致
- 之后能读取到的话就是最终一致（BASE理论）

**Availability（可用性）**

集群中一部分节点故障后，集群整体是否还能响应客户端的请求（有数据就行，不管它对不对）

**Partition tolerance（分区容忍性）**

系统中任意信息的丢失或失败不会影响系统的继续运作

**CAP三个指标三者不可得兼，只能三选二**

~~P是必须满足的~~

举例说明：如果选择了 CA 放弃了 P，当发生分区现象时，为了保证一致性必须拒绝请求，但是 A 又不允许；因此 P 是必须满足的

如何保证P？做数据冗余，例如 mysql、redis、zookeeper

~~为什么不能同时满足CA？~~

假设用户向服务1 发送了写请求更改了数据，由于网络断开，这个更新同步不到服务2
此时如果又有一个读请求发给了服务2
1、读取成功但是读到的不是最新数据（不满足一致性）
2、阻塞等待直到网络恢复，服务2 数据得到同步之后再返回给用户（不满足可用性）

~~CP、AP 如何取舍？~~

看业务的容忍度

不错的策略是保证 AP 兼顾C（舍弃强一致，保证最终一致）

**BASE理论**

BASE 是这三个短语的缩写

- Basically Available（基本可用）（允许损失部分可用性，但绝不是不可用）
- Soft state（软状态）（允许请求与数据同步成功之间存在中间状态）
- Eventually consistent（最终一数性）

其核心思想就是：即使无法做到强一致，应该采用适当的方式达到最终一数性

# 🥇负载均衡算法

**随机**

**轮询（Round Robin）**

**加权轮询（Weight Round Robin）**

服务器硬件配置不同的情况下使用，通过概率实现权重不同
比如【1-10】取随机数，如果取到【1-6】就找6权重的服务

**散列取模**

可以将请求的 IP 地址转化为散列值，与服务器数量取模，选择特定的服务器，此算法绑定请求者的 IP 与服务器

**最少链接**

记录每个服务器正在请求的链接数，将请求分发到链接数量最少的服务器

# 🥇限流算法

## 固定时间窗口算法

在指定的时间周期内，访问次数达到阈值时触发限流策略
下一个时间周期进行访问时，访问次数清零

*缺点*有一个临界值问题，也叫突刺问题；下面举例说明

假设阈值是100

- 1点-2点 是一个时间周期，周期内访问次数没有超过阈值，后半小时访问次数是60
- 2点-3点 是一个时间周期，周期内访问次数没有超过阈值，前半小时访问次数是70
- 那么 1点半-2点半 这个时间周期内访问次数是 130 已经超过阈值了，但是不会触发限流策略

## 滑动时间窗口算法

滑动时间窗口是将时间周期切分为多个小的时间窗口，分别计算每个小窗口中的访问次数，最终统计求和

能很好地解决临界值问题

小窗口划分越多，限流的统计就会越精确

## 漏桶算法

想象一个漏斗，出水口速率恒定，入水速率不一定。

漏桶算法的特点：

- 漏桶具有固定容量，固定的流出速率（流出请求）
- 入口流量可以以任意速率流入（流入请求）
- 如果入口流量超出了桶的容量，则流入流量会溢出（新请求被拒绝）

**优点**突发请求时丢弃的请求较少
~~缺点~~无法精确控制流出速度；无法调整桶的大小

可以用于瞬时高并发流量，比如秒杀

## 令牌桶算法

令牌桶算法的特点：

- 桶中存放的是令牌而不是流量，令牌桶满了令牌被丢弃
- 每当一个请求过来时，就会尝试从桶里移除一个令牌，如果没有令牌的话，请求无法通过

**优点**可以通过控制令牌的放入速度动态调整请求的处理速度
~~缺点~~突发请求时如果令牌的放入速度调整不及时，可能会丢弃很多请求

可以用来控制自己的处理速度，防止突发请求导致自己过载
也可以用来控制自己请求第三方服务的速度，防止把下游压垮

# 🥇分布式幂等性如何保证

类似的问题：重复消费、重复提交

什么是幂等性？

两次操作造成的结果无差别

解决方案

1、数据带上版本号（对业务有入侵，不推荐）
2、数据库中新建去重表，将消息ID插入，只有一个能插入成功（Redis setNx 也可以实现）
3、重复提交可以使用 token 解决，提交时 token 失效下发新 token

# 🥇分布式ID

## 分布式ID应具备的特点

- 全局唯一
- 高性能
- 高可用
- 简单易用
- 递增：局部递增即可

## UUID

优点：

- 全局唯一
- 简单易用，JDK自带可以不用依赖于任何第三方服务

缺点：

- 完全无序，插入数据造成频繁的页分裂，数据量大时严重影响性能

## 数据库

利用数据库的自增主键

### 单机（可用性差+性能低）

如果使用 mysql 单实例：性能可能不够；单实例的稳定性不强，宕机会影响业务

### 解决高可用

**方案一：使用主从模式集群，做到高可用**

存在的问题：数据同步延时问题，宕机可能导致ID重复

**方案二：使用多主集群（相互无关联），如果一个主宕机获取ID失败，就去请求另一个主，同时报警提醒**

举例：比如两个主机 master1 只产生奇数ID，master2 只产生偶数ID

- 必须设置自增起始值与步长
- 不能要slave，因为故障切换可能引起ID重复问题

存在的问题：
1、性能可能依旧不足，每次都去请求数据库
2、如果再新增一个主，扩展起来麻烦

### 号段模式解决性能问题

**方案三：使用号段模式，批量从数据库获取ID缓存起来，缓存中的ID一旦达到上限时再去数据库批量获取**

DB设计如下：

> biz_type：代表业务类型，用作业务隔离
> max_id：代表当前最大的可用id
> step：代表号段的长度，合理设置即可
> version：是一个乐观锁，用来保证并发更新的正确性

<img src="C:\ImageA\image-20231030114731055.png" alt="image-20231030114731055" style="zoom:50%;" />

### 高可用+高性能

结合方案二和方案三的优点，DB设计如下：

> delta：表示缓存中的 ID 每次增量
> remainder：代表余数

<img src="C:\ImageA\image-20231030114706908.png" alt="image-20231030114706908" style="zoom:53%;" />

仍然有缺点
1、扩展主机个数麻烦
2、实现复杂
3、依赖数据库

开源的实现方案有《美团Leaf》《滴滴TinyId》

## Redis

可以采取类似上文数据库的解决方案，而且因为 Redis 性能极高，因此不用方案三，使用方案二就能够解决问题了

仍然有相同的缺点
1、扩展主机个数麻烦
2、依赖Redis

## 雪花算法

使用一个64位的 long 类型作为 ID

- 第一位0表示正数 + 41位毫秒级时间戳 + 10位主机编号 + 12位序列号
- 时间戳通常是相对时间，这样可用期限就更长，理论上支持 69 年
- 支持 1024-1=1023 个主机节点
- 支持同一个时间点同一台服务器生成 4096-1=4095 个序列号
- 主机编号的位数和序列号的位数可以做适当调整

存在的问题：
1、时钟回拨可能造成ID重复，可以缓存最大时间解决这个问题，见《美团Leaf》
2、主机编号手动分配太麻烦了，可以在 Zookeeper 中创建序列节点，用节点序号作为主机编号

开源的实现方案有《美团Leaf》，而且使用 Zookeeper 生成序号取回之后缓存在本地，对 ZK 是弱依赖

# 🥇分布式事务

## 两阶段提交（2PC）

> 事务参与者：每个数据库就是一个事务参与者（RM：资源管理器，通常与事务参与者同义）
> 事务协调者：访问多个数据源的服务程序，就是事务协调者（TM：事务管理器，通常与事务协调者同义）

两阶段提交协议，是将整个事务流程分为两个阶段：准备阶段、提交阶段
2PC：2是指两个阶段，P是指准备阶段，C是指提交阶段

1. 准备阶段：事务协调器要求每个涉及到事务的数据库预提交此操作，并反映是否可以提交
2. 提交阶段：事务协调器要求每个数据库提交数据，其中如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚

**XA协议**是一个两阶段提交协议，MySQL 从5.5开始支持，SQL Server 2005 开始支持，Oracle7 开始支持

优点：强一致性、成本低

缺点：

- 单点问题：如果事务协调者宕机，事务可能永远无法完成，资源将会一直阻塞
- 资源阻塞，事务参与者在执行SQL与提交事务的时间窗口内会锁住资源，想要使用这些资源的事务只能等待
- 数据不一致：事务协调器通知事务参与者提交，如果因为网络原因只有一部分事务参与者收到了消息，就会造成数据不一致

**Seata 是一个分布式事务框架**

- XA模式是对 XA协议的实现，是强一致性的
- AT模式在准备阶段提交事务+记录UndoLog，提交阶段什么都不做或回滚，解决了 XA模式资源阻塞问题

## TCC

TCC 是一种编程式分布式事务解决方案，要求每个从服务提供三个接口：Try、Confirm、Cancel

- Try：主要是对业务系统做检测及资源预留
- Confirm：真正执行业务，只使用 Try 阶段预留的业务资源
- Cancel：释放 Try 阶段预留的业务资源

【第一阶段】主服务调用所有从服务的 Try 操作并得到返回结果
【第二阶段】所有 Try 操作都成功就调用所有从服务的 Confirm 操作，只要有一个 Try 失败就调用所有从服务的 Cancel 操作

缺点：

- 需要编写大量补偿事务的代码
- 代码入侵非常强

Seata 也提供了 TCC 模式

# 🥇分布式锁

## 数据库

- 悲观锁的实现：使用排它锁，select for update
- 乐观锁的实现：可以通过 version 字段实现，也可以使用唯一索引实现

**缺点**大量请求拿不到锁不停重试，造成对数据库的压力
**缺点**而且数据库有宕机风险，主从也不行可能丢失数据

## Redisson

基于 Redis SETNX 实现分布式锁，遇到的问题以及解决方案：

1. 死锁？设置过期时间
2. 锁过期，其他线程获得了锁怎么办？守护线程，自动续期
3. 锁被别人释放？锁写入唯一标识，释放锁先检查标识，再释放
4. 可重入锁？还需记录重入次数，上面需记录唯一标识，因此简单的 KV 不行，可以用 hash hset

代码实现参考《语雀文档》

PS：上面这些 redisson 都已经实现了

**问题：未抢到锁的线程如果轮询抢锁，可能造成很多无效的IO**可以用基于发布订阅的回调方式抢锁，这点 redisson 也实现了
**问题：单节点Redis 做不到高可用**可以使用哨兵主从，但是主从复制是异步的，可能出现主加锁后挂了，没有同步到从，其他线程可以获得锁

与数据库相比就是速度快，因此有了这种方案是绝对不会使用数据库的。

## RedLock

| <img src="C:\ImageA\image-20231030114756508.png" alt="image-20231030114756508" style="zoom: 80%;" /> | <img src="C:\ImageA\image-20231030114816439.png" alt="image-20231030114816439" style="zoom: 67%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

**举例说明时钟漂移**

1、客户端1获得了A、B、C节点上的锁，由于网络问题，无法到达D和E
2、节点C上的时钟向前跳动，导致锁过期
3、客户端2获得了节点C、D、E的锁，由于网络问题，A和B不能被联系到
4、客户端1和2现在都认为他们持有锁
也或者，在第二步骤，节点C如果出现宕机，恢复后没有之前的数据，客户端2也可能获取到锁

如果应用追求高性能就用 redisson（哨兵主从，只有很小几率发生锁丢失)
如果应用想要保证正确性，可以使用 zookeeper

## Zookeeper

**加锁和解锁过程**

1. 线程在目录下创建一个临时序列节点
2. 线程获取目录下的所有节点
   1. 如果它刚才创建的节点是最小节点，我们就认为这个线程获得了锁
   2. 如果不是最小节点，这个线程阻塞，同时监听比自己小的节点的删除事件
3. 线程结束删除这个节点，相当于释放锁，唤醒监听删除事件的线程获得锁（客户端断开连接也删除节点）

**能够解决 Redis 所有的痛点**

- 死锁：临时序列节点能确保不死锁
- 重复锁：客户端保持连接锁就存在，连接断开节点删除锁删除
- 抢锁线程主动轮询：有 watch 机制
- 羊群效应：删除事件唤醒大量线程抢锁？只 watch 比自己小的节点
- 单机不高可用：zookeeper高可用
- 主从同步数据不一致：zookeeper 最终一致性