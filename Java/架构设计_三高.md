# 🥇什么是三高

高性能、高并发、高可用

# 🥇如何做好架构

靠代码是做不到三高的，必须依靠架构

好的架构，一下子拿出来是不切实际的，一定是演化来的，是迭代出来的

没有完美的解决方案，懂得做取舍

不要过度复杂化系统

# 🥇系统衡量指标

## 🥈性能指标

**吞吐量**

单位时间内，能接受和发出的数据量

~~TPS~~Transaction Per Second 每秒处理的事务量（不是数据库的那个事务，举例：首页展示就可以作为一个事务）
~~QPS~~Queries Per Second 每秒查询量
面试时候给一个数就可以了
它们之间的换算关系：必须由指定的业务场景来定

而且业务不一样，导致很难通过吞吐量比较两个系统的性能高低；只能对比自身不断优化

**并发数**

> 并行：在某个时间点，多个任务进行
> 并发：在某个时间点，只有一个任务进行；但是在一个时间段，有多个任务同时进行（举例：一边看书一边看电视）
> 大量并行和并发都会使服务压力变大

并发用户数：一部分用户使用业务，另外一部分用户只是登录挂机没有具体操作
并发连接数：用户和服务器之间的连接；一部分连接在传输数据，一部分连接仅仅是连接而已
并发请求数：有的请求静态数据，有的是读写操作
并发线程数：并发的线程数目，A系统并发100都是请求静态资源，B系统并发50但都是写操作，难以比较

并发数不能清晰准确的表示系统性能

**响应时间**

响应时间：RTT

平均响应时间：所有响应时间的平均值

## 🥈可用性指标

发生故障的概率 = 故障时间 ➗ 总时间

可用性指标可以用 不发生故障的概率 来衡量

提高可用性就是降低故障发生的概率

# 🥇客户端优化

## 🥈减少不必要的传输

### 🥉减少传输数据量

不必要的 cookie 不传输

**图片**如果允许使用缩略图，就使用缩略图，减小体积（查看图片时才看到原图）

**HTML、JavaScript、CSS**删除无效字符，语义合并，减小体积

**HTTP请求压缩**
head 中加参数：`Accept-Encoding: gzip, deflate`，表示客户端 可以接受的压缩内容的格式
服务端响应：`Content-Encoding: gzip`

### 🥉减少传输次数

**JS、CSS**文件合并

**图片**
1、雪碧图，请求多次转化为请求一次
2、base64图片，不用请求了

**合并 HTTP 请求**

**页面缓存**

可以控制客户端、各级代理对页面资源的缓存

head 中加参数：`cache-control: public`

> pubic（在响应中设置）：各级都能缓存
> private（在响应中设置）：只能客户端缓存，中间各级不缓存
> no-cache（请求响应都可以设置）：可以缓存，但是不能直接使用缓存，要去服务端验证一下
> no-store（请求响应都可以设置）：都不能缓存

缓存有效期：

> max-age=秒，缓存可以存活的时间
> s-maxage=秒，在各级节点存活的时间，如果是客户端存储忽略
> max-stale=秒，可以忍受资源过期的时间
> min-fresh=秒

重新验证和加载的设置

> must-revalidate：资源过期，服务器重新验证之前，不可以使用该资源
> proxy-revalidate：各级节点有效
> no-transform：不能压缩图像
> only-if-cached：只要缓存资源，不要服务器资源

缓存更新不及时

方案1：更新文件名
发布上线一个文件，这个文件名是变的，那么缓存的文件就失效了

方案2：验证缓存的有效性
1、基于文件的最后修改时间
2、基于版本号

**尽可能多做静态化**

## 🥈懒加载

树形组件、标签页，到了不得不看具体数据的时候，才调用后端
但是也不一定，一些经常不变的数据，就没必要了

## 🥈预加载

**同一个域名下**

```html
<!-- 针对当前页面，更早的去下载资源，当前页面的资源 -->
<link rel="preload" href="xxxx.js" />

<!-- 针对下一页，当前页面处理完，浏览器闲置的时候，会去加载该资源 -->
<link rel="prefetch" href="xxxx.js" />
```

**不同域名下**

```html
<!-- 打开域名预解析，在当前页面，完成对下一个页面域名的解析，而在下一个页面直接使用预解析之后的结果 -->
<meta http-equiv="x-dns-prefetch-control" content="on">
<link rel="dns-prefetch" href="//www.baidu.com">

<!-- 针对当前页面，更早的去下载资源，当前页面的资源 -->
<link rel="preload" href="xxxx.png" />
<!-- 针对下一页，当前页面处理完，浏览器闲置的时候，会去加载该资源 -->
<link rel="prefetch" href="xxxx.js" />
```

## 🥈其他

请求头中`connection: keep-alive`用的长连接，长连接可以复用，一次连接可以请求多次
减少了创建和销毁连接的消耗

# 🥇CDN

**DNS**

全名：Domain Name System

作用：将域名解析成IP地址

DNS 记录的信息

- A（Address）记录：域名和IP的对应关系（一个域名可以通过 DNS 负载均衡到多个IP地址）
- CName（Canonical Name）记录：域名和域名的对应关系
- NS（NameServer）：域名和域名解析服务器的对应关系，我解析不了，转发给另一个 DNS 服务器解析

**CDN原理**

CDN 全称 Content Delivery Network

在域名和IP 之间做了一道拦截

1、用户请求，通过 DNS 的 CName 解析到 CDN 域名
2、请求转发到 CDN 域名得到网络拓朴最近的 CDN 节点，节点如果有数据就返回，如果没数据就去请求源站拿到数据后返回

**CDN的作用**

1. 将请求发到距离用户【网络拓扑】最近的服务器，降低网络延时，减少响应时间
   CDN节点越多，响应时间越少，但成本也会提升
2. 可以缓存静态内容（图片、视频、附件），以后不再请求服务，减少了请求源站的并发数
   配置规则列表决定哪些数据需要缓存
   对于动态内容，还是需要请求源站的
3. 多个 CDN 节点可以起到分流的作用（对于静态内容有效）
   但是对于动态内容，是先分再聚，请求最终还是打在源站上，相当于没有起到分流作用

**缺点**

1、部署成本增高，数据的同步会复杂，可以找 CDN 服务商解决
2、对于动态内容，CDN 对于源站是没有起到分流作用的

# 🥇对源站做分流

CDN 相当于静态内容的横向扩展

源站也可以做横向扩展：服务集群，可以通过【多地址直连】或【代理】实现

源站还可以做纵向扩展：分布式系统

上述两个横向扩展不仅利用分流实现了高性能和高并发，也做到了高可用

## 🥈分布式系统

服务集群是水平复制，也就是 AKF 原则的 X 轴

还有一种分流方式是拆应用，可以按功能或业务不同拆分为多个应用，即 AKF 原则的 Y 轴

拆了应用如果还是扛不住，可以对每个应用再横向扩展

## 🥈服务集群

注意点

- 注意幂等
  什么是幂等？第一次以后对资源的操作不会对资源产生副作用
- 注意数据存储的共享

### 🥉无状态节点集群

无状态：服务端不存储任何跟请求相关的数据
有状态：在服务端存储之前的请求信息，用于后面请求的处理，比如存储session

我们跑代码的服务一般用无状态集群，涉及到存储数据，可以使用公共存储，实现无状态

对于减轻服务器压力来说，无状态可以方便的扩缩容，因为所有节点都一样

**注意集群内的定时任务**

如果每个节点都跑一样的定时任务，任务就重复了
举例：某班次列车出发前一个小时给所有的乘客发送短信提醒，会出现重复发送短信的问题

方案1：在集群内使用分布式锁，例如`redis setnx`，给某个用户发短信前加锁，然后判断某个 key 是否存在，存在的话就不再发送短信，不存在的话就发送短信，发送成功之后新 set 一个 key，然后释放锁，另一个等待的线程将获得锁

方案2：也可以把定时任务放在外部，将每个发送短信的子任务发给负载均衡器，让服务节点分摊发送就行了

**关于公共存储**

共享存储（最常用的）：存储容量限制，读写性能没有读写分离架构好，单点故障问题
伪共享（主从读写分离）：数据一致性问题，强一致 or 最终一致根据业务做取舍（CAP）

### 🥉有状态服务节点集群

**场景1**

存储session

要求：
1、为用户分配特定的服务器才能维持会话（分配策略：用户ID hash取模 或者 用户ID分段）

**场景2**

游戏选区
每个区拥有固定的游戏人

要求：
1、用户手动选择服务器
2、为用户自动分配特定的服务器（分配策略：用户ID hash取模 或者 用户ID分段）

缺点：
1、容错性差，但是服务挂了的话就出问题了
2、扩展不容易

## 🥈多地址直连

多地址直连，可以解决源站的分流问题

**方案1**可以做一个注册中心，参考下面步骤：

1. 地址获取：客户端通过注册中心拿到服务IP
   1. 注册中心做分流算法，将算法得到的IP交给客户端请求
   2. 注册中心直接交给客户端一个IP列表，由客户端自主选择服务器（举例：网通下载、联通下载、电信下载）
2. 内容请求：请求实际服务

缺点：需要做一个注册中心，挺麻烦的

**方案2**在客户端本地做一个规则中心

规则中心包括 IP 列表和分流算法

客户端可以通过分流算法自动选择服务，也可以由用户自主选择服务

对比方案1
1、不用做注册中心：成本低（优点）不能动态改变服务器信息（缺点）
2、少了一次地址获取的请求，速度会更快（优点）

## 🥈代理与负载均衡

反向代理也可以做到分流作用

正向代理：代理特定的用户去访问其他东西（举例：VPN）
反向代理：代理特定的服务被用户访问（举例：Nginx7层反向代理）

4层反向代理：基于 TCP 协议，越靠近底层，知道的信息越少（IP、端口），IP端口转发效率高
7层反向代理：基于应用层协议（https、ftp），知道的信息更多（方法、头、正文、cookie），更智能，效率低

负载均衡算法：TODO

我们利用反向代理，将请求分流到服务集群

# 🥇服务内的并发

## 🥈多线程

可以使用异步线程，加快响应，提升并发数

## 🥈线程数的计算

线程数 = cpu核数 * cpu利用率  * (1+w/c)

cpu利用率：0-1之间，算的时候用100%
w：wait 等待时间
c：computer 计算时间

线程数 = cpu核数 * (1+w/c)

直观的结论：等待越久，线程数越高

实际【以压测为准】

# 🥇缓存设计

对于跑代码的服务集群（无状态），我们可以无限的横向扩展解决高并发和高性能的问题

但是对于共享数据库来讲，性能是有瓶颈的

缓存**用空间换时间**，是数据库的前置节点，减轻数据库的压力

适合读多写少的场景
适合耗时特别长的查询

## 🥈缓存更新机制

### 🥉准确性和实时性要求不高

查数据，如果缓存中没有就查库+写缓存（有过期时间），如果缓存有数据就用缓存数据
写数据，直接写数据库

适合对数据准确性实时性要求不高的场景，比如商品关注的人数、视频的点赞数

### 🥉准确性和实时性要求较高

这种情况下就需要保证双写一致性了

我们讨论四种情况

标签：`#双写一致性`

#### ✨先更新缓存，再更新库

这个方案我们一般不考虑

原因是更新缓存成功之后，更新数据库出现异常了，导致缓存数据与数据库数据不一致
数据库更新异常回滚缓存？做不到吧，即使能做到实现也是比较麻烦的
数据库更新异常删除缓存？这等价于下面将要提到的方法

#### ✨先更新库，再更新缓存

这个方案我们一般也不考虑

原因是数据库更新成功了，缓存更新失败，同样会出现数据不一致问题
缓存更新失败回滚数据库？有些业务是会受影响的，我们应该保证的是落库成功就是成功，缓存最终会和数据库保持一致
缓存更新失败删除缓存？等价于下面将要提到的方法

除此之外这个方案在并发场景下存在数据不一致的问题，举例如下

1. 线程A更新了数据库
2. 线程B更新了数据库
3. 线程A更新了缓存
4. 线程B更新了缓存

实际情况是 1先于2，但是由于网络原因 4先于3，此时就是数据不一致

#### ✨先删缓存，再更新库

此方案在并发场景下也有数据不一致的问题，举例如下：

1. 几乎同时一前一后来了两个请求，A是更新 B是查询
2. A 先删除缓存，然后去更新数据库；此时B 看到缓存是空的就去查数据库
3. 正常的情况是A更新成功，然后B读到新数据并写入缓存
   但是读大概率比写快，很有可能的情况是B读到旧值写入缓存，然后A更新成功

如何解决呢？**延时双删策略**，如下

1. 先删除缓存
2. 再更新数据库
3. 睡眠1秒，再次删除缓存

延迟双删可以将1秒内的脏数据再次删除
这个1秒需要根据业务计算，一般是在读操作的耗时基础上加上几百毫秒
计算得到的睡眠时间也不是100%就能保证数据一致，因此可以根据业务需求适当睡眠时间长一些提高这个概率

还存在一个问题？如果数据库是读写分离架构，数据同步也有一定时间差
解决方案1：还是用延迟双删，睡眠时间为主从同步的延时时间基础上加上几百毫秒
解决方案2：让查询走主库

**延时双删同步异步的问题**

如果延时双删失败了怎么办？重试
假如延时双删是同步的，多次重试将严重影响系统的吞吐量；而且延时双删本身需要先睡眠一定的时间，也会影响吞吐量
因此延时双删应该做成异步的，另外启动一个线程，异步删除
延时双删优雅的做法是使用 canal 订阅 binlog + 删除，如果删除失败就发送到 MQ 由消费者再次尝试删除缓存
这个方法的优点是延时双删和业务代码解耦

**延时双删还存在的问题**

1、第一次删除缓存如果失败了不好处理
2、而且两次删除没有必要，保留最后一次就可以了

因此综合来看下面第四种的方案更好一些

#### ✨先更新库，再删缓存❤️

> 这也是经常采用的方式

使用上面的延时删除策略，去掉第一步的删除缓存即可：

1. 更新数据库
2. 睡眠一定时间之后删除缓存

此方案在并发场景下也有数据不一致问题（前提：缓存无数据，数据库有数据），举例如下：

1. A查询，B更新
2. A查缓存无数据，去读数据库读到旧值
3. B将数据库更新为新值并删除缓存
4. A将旧值写入缓存

只有在满足前提，且读比写慢的情况下才会出现，概率极低

**存在的问题**

1. 删除缓存失败怎么处理
2. 上述概率极低的情况处理

延时双删可以同时解决这两个问题

1. 更新数据库
2. 删除缓存（不用关心成功还是失败）
3. 延时双删

### 🥉允许数据丢失的情况

把缓存当作数据库来用，业务线程只写缓存不写数据库，异步将缓存同步到数据库

速度极快

## 🥈缓存清理机制

**原则**

尽可能提高缓存命中率

**过期式清理**

定时轮询删除 + 惰性删除，参考《Redis#过期式清理》

**数据阈值式清理**

参考《Redis#数据阈值式清理》

采用恰当的清理算法能够提高缓存命中率

阈值越大命中率越高

## 🥈缓存风险

穿透、雪崩、击穿

参考《Redis#缓存穿透、击穿、雪崩》

**缓存预热**高频访问的数据提前准备，在服务启动过程中加载到缓存，避免击穿，比如商城首页的商品类型多级菜单

# 🥇数据库设计

## 🥈单节点的优化

范式与反范式

索引

还有其他手段，参考《MySQL，TODO》

但是，单节点数据库性能有瓶颈，巨量数据的情况下需要采用另外的手段（分流），下文将介绍

## 🥈表分区

每个表对应磁盘上的一个 ibd 文件，表分区其实就是分流，对一个 ibd 文件的请求，分发到对多个 ibd 文件的请求

逻辑上还是一个表，物理上分开存储

好处：业务代码不用改！如果表分区能够满足我们，就没必要分表增加编程的复杂度了

**分区策略**

- range：范围分区，范围
- list：列表分区，等值
- hash：hash值 与 分区数量 取模
- key：主键 or 唯一键分区，就是 hash 分区，只不过指定了列
- 子分区：分区基础上再分区，嵌套

**注意事项**

- NULL 值无法命中分区
- 分区列必须放到 where 语句中才能命中分区
- 结合查询规则，尽量保证常用查询落到一个分区中

## 🥈分库分表

**目的**

通过分库分表应对高并发

**分库分表的设计**

一个库里有两张表A、B，如何分？
1、不破坏表：A放一个库，B放一个库（业务拆分时使用）
2、破坏表：垂直分和水平分（原则：避免跨表操作）

每个表数据少了，提升单表查询效率；再分库，降低单点库的压力

**分库分表的方法**

- 按范围划分
  分段小：子表数量多，增加维护难度；分段大：单表依然有可能存在性能问题
  分的依据：分表后，表的各方面性能，能否满足系统要求
  优点：可以平滑的扩充新表，只需增加子表的数据量，原有的数据不用动
  缺点：数据分布不均匀（不算啥了）
- hash路由：hash取模
  优缺点：与范围划分相反

**分库分表的问题**

ID需要全局唯一，因此需要采用分布式ID

跨库的事务问题：采用分布式事务

拆分维度的问题：根据自己业务的情况，看哪个字段用的多

成本问题：非必要，不分库，不要过度设计

**好文章**

[分库分表及常见解决方案总结](https://blog.csdn.net/weixin_41668084/article/details/113356941)

[详谈为什么互联网公司需要垂直分表](https://blog.csdn.net/weixin_46785144/article/details/120091357)

## 🥈读写分离

分库分表是 AKF 原则的 Y 轴（业务拆分）或者 Z 轴（数据分片）
读写分离是 AKF 原则的 X 轴，相当于横向扩展

适用于读多写少

**读场景主从复制延迟的问题**

~~方案1~~
先查从库，查不到的话再查一次主库，如果还没有的话那就真的没有了

~~方案2~~
主业务用主库，非主业务用从库
举例：注册登录都走主库，用户介绍走从库

# 🥇高可用性

## 🥈横向扩展（复制）

即使有节点故障
如果负载均衡器拿到的都是健康的节点，请求还是可以被正常处理的，能保障可用性
即使负载均衡器负载的节点有故障节点，横向扩展也可以极大的提高可用性

## 🥈化串联为并联

串联指标：`99% * 99% * 99% * 99% * 99%`越乘越小
并联指标：`1 - (1-r1) * (1-r2) * (1-r3)` 越乘越大

增加并联支路，消除单点依赖（举例：服务集群）

化串联为并联（举例：把 Read Write Through 模式转化为 Cache Aside 模式）
Read Write Through（调用方只和缓存打交道，而缓存负责保证自身和放据提供方一致）（串联）
Cache Aside（调用方先读缓存，如果没有去读数据库）

## 🥈隔离

数据隔离：核心业务与非核心业务，使用不同的数据库

机器隔离：给重要的用户单独配置服务器，根据用户标识去路由（比如银行要优先保证大客户的操作体验）

集群隔离：服务分组（注册中心），例如单独分出一组服务给秒杀业务，避免被别人影响和影响别人

线程池隔离：不同的业务用不同的线程池，重要的业务不要被边缘业务影响

机房隔离：防止机房断电导致服务不可用

读写隔离：主从

## 🥈降级&熔断&限流

**概念不同**

~~降级~~
当【自身】【响应慢或异常达到阈值】，为了防止自身全面崩溃，触发降级策略，【目标是保证核心业务可用】
恢复（sentinel）：同熔断

~~熔断~~
当【下游服务】【响应慢或异常达到阈值】，【断开】不再调用下游，快速返回
对于自身，避免因为下游故障导致自身响应慢甚至影响自己的上游服务
对于下游，暂时熔断减轻下游的访问压力，下游得到了保护

恢复（sentinel）：经过熔断时长后半开，放行一次调用，如果还是【响应慢或异常】就继续熔断，否则解除熔断

~~限流~~
是一种过载保护，限制【外部】流量，比如QPS，达到阈值就限流；sentinel 还可以限制内部线程数
流控效果1（sentinel）：达到阈值，快速失败
流控效果2（sentinel）：预热，逐步放开请求，防止突然激增的请求压垮系统，比如缓存预热，防止大量请求打在数据库上
流控效果3（sentinel）：达到阈值，排队等待

**降级策略**

- 停止读数据库，去读缓存，准确结果转为近似结果（举例：商品已销售数量）
- 直接显示静态结果（举例：用户推荐由动态改为静态，比如猜你喜欢）
- 功能裁剪：干掉非核心业务
- 禁止写：高峰期减少不必要的写
- 分用户降级：银行保留对大客户的服务，降级对普通用户的服务
- 工作量证明：验证码、数学题、拼图题、滑块，排除恶意访问，例如12306登录

**手动降级**

方案1：系统提供后门接口
方案2：开发独立的降级系统

前提：代码提前规划好

**自动降级**

提起写好触发降级逻辑

**限流算法**

看上文，TODO

## 🥈异步&解耦&削峰

不同于 降级&熔断&限流

异步&解耦&削峰 是将暂时处理不了的请求暂存起来，慢慢处理

## 🥈冗余设计&容错

### 🥉主备&故障切换

mysql、redis、ES

### 🥉异地多活

本质就是灾备

其核心是通过数据冗余来保障两份数据是完全一样的

**服务不可用的场景与解决方案**

| 服务不可用场景               | 解决方案                     |
| ---------------------------- | ---------------------------- |
| 机房故障，比如机房断电       | 同城异区                     |
| 全城故障，比如全城停电、地震 | 跨城异地（没有必要跨国异地） |

**数据不一致的问题**

【举例：注册】

A中心注册了用户，数据还未同步到B中心，此时A宕机，B启用，用户重新注册在B中心注册成功，如果A中心恢复，B中心的数据同步到A中心时就会有数据冲突

即使能保证数据最终能够同步到B中心（有一定延迟），因为用户在B中心注册成功了，A中心同步过来的数据也是有冲突的

这种数据冲突是无解的

注意：不能为了架构设计去改变业务规则

很多场景是无解的，保证全部业务多活是不可能的
原则1：保证核心业务多活
原则2：保证核心数据最终一致性，允许不是实时的（肯定不是实时的）
原则3：保证大部分用户可用，小部分用户因为数据没有实时同步不可用

**异地多活设计步骤**

1、业务分级，保证核心业务多活
2、多通道数据同步，保证核心数据最终一致性，类似携程那个方案

## 🥈监控与告警

# 标签

标签：`#三高架构`